<NeuralNetwork>
    <Inputs>
        <InputsNumber>12</InputsNumber>
        <Item Index="1">
            <Name>position.x</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="2">
            <Name>position.y</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="3">
            <Name>position.z</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="4">
            <Name>view.x</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="5">
            <Name>view.y</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="6">
            <Name>view.z</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="7">
            <Name>source.x</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="8">
            <Name>source.y</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="9">
            <Name>source.z</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="10">
            <Name>normal.x</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="11">
            <Name>normal.y</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="12">
            <Name>normal.z</Name>
            <Units></Units>
            <Description></Description>
        </Item>
    </Inputs>
    <ScalingLayer>
        <ScalingNeuronsNumber>0</ScalingNeuronsNumber>
        <ScalingMethod>MinimumMaximum</ScalingMethod>
    </ScalingLayer>
    <MultilayerPerceptron>
        <Architecture>12 20 10 3</Architecture>
        <LayersActivationFunction>HyperbolicTangent HyperbolicTangent Linear</LayersActivationFunction>
        <Parameters>-1.40551 0.68892 -1.12181 2.04237 -1.17554 1.67448 0.994357 -0.708083 -0.539181 0.861598 0.416373 -2.00374 0.0969984 1.03446 -0.403453 0.981265 2.1863 -1.40324 -0.246615 -1.04391 0.0759079 -0.30078 0.96946 -0.97947 -0.015084 -1.08406 0.0833179 0.679735 -0.774737 -2.14404 -1.90513 -0.830957 0.582602 0.879911 0.963776 1.24722 -1.7863 0.356126 0.272407 -1.31585 -2.81687 1.32902 -1.93501 0.861704 0.406213 0.137229 -0.933735 0.593555 -0.303402 0.26406 0.959287 -0.469684 0.452058 -0.370363 1.71668 2.16686 -0.488901 -0.296275 -0.846477 -0.250338 -0.745517 -0.778642 0.267289 -0.154101 -0.348158 1.26447 1.607 -0.273524 -2.99503 1.50103 1.74851 -0.498515 -0.200469 -0.18966 0.00498298 -0.883475 0.874954 1.05073 0.164785 -0.457216 -1.01748 -0.251737 -1.19014 0.805282 0.0563307 1.37792 0.606279 0.432179 -0.685155 -0.894478 0.00116031 -0.0223446 0.0253538 1.0245 0.494663 0.551783 1.67644 -1.70208 0.101912 -0.26107 0.869506 0.640091 -0.979597 1.58489 -0.802939 -1.22245 -0.137716 1.18253 -0.559064 0.323799 -1.39343 -1.51525 0.163955 -1.73449 0.521046 -1.0802 -0.782461 0.72646 -0.180214 0.81291 2.72372 0.0698483 -1.74686 0.822734 0.160011 0.857763 1.56445 -0.340253 0.37038 0.0839285 0.930665 -0.072102 0.607129 0.399865 -0.600929 -1.17408 -0.121637 -1.72517 -1.35984 -0.24286 1.05297 2.01105 -0.135175 0.264716 -0.141071 1.20939 0.470001 0.514303 -1.39848 -0.576214 -0.759591 1.36285 -0.432047 0.0454815 1.63913 -1.56748 0.210419 -0.540961 0.8369 1.6435 -1.19456 -1.06472 1.13558 0.0325321 1.37744 -0.648478 0.271298 0.409026 -0.32197 0.0530112 0.00970636 -0.248516 0.367935 -1.21919 0.97975 0.7042 -0.218677 3.19714 -0.861955 0.583363 -0.760493 -1.26746 0.0216212 -0.710539 -1.31958 2.2332 -2.27579 -1.29541 0.528672 0.429887 1.25135 0.0791579 2.28581 -0.822881 1.71503 1.94098 1.66397 -1.0801 0.137506 0.498711 -0.739107 0.884578 -1.47655 0.328246 -0.0997598 -0.417394 0.843579 0.114229 0.400682 0.307454 2.13832 1.6451 -0.574651 -1.59399 -0.259946 -0.926775 0.0938701 -0.413225 1.15822 -0.702073 -0.127381 -1.10227 0.607068 -1.39743 -0.471449 0.207609 -0.00813303 -1.14271 2.24108 -0.434078 0.18461 0.808287 -0.0194157 -0.983022 -0.129039 -2.45782 -0.696549 -1.82628 -0.229573 0.350096 -0.965281 -0.288834 0.502029 0.58792 -0.641995 0.431435 -1.08535 -0.364606 1.08001 0.652846 0.697244 0.390227 -1.12521 -2.63315 -0.483681 -0.290068 -0.702023 0.695653 -1.25042 -1.48997 1.03546 0.454913 0.518669 -0.713983 -1.33765 1.75742 -1.26345 0.938916 1.65065 -1.04201 1.95609 -1.0251 0.753463 1.17379 0.93362 2.79777 1.13122 0.992566 0.381833 -1.43768 -0.252205 1.11474 -0.401789 0.409499 1.6214 0.349358 1.51577 0.423704 -0.997207 0.231099 -0.160371 0.245914 1.49148 0.604924 -1.17935 -0.178903 -0.269368 -0.652014 0.417311 0.625549 -0.113683 0.645986 -0.915857 1.45102 -1.77904 1.85217 2.01146 -0.794524 3.48458 1.52444 1.48538 -0.754614 -0.306297 1.92939 -0.0609458 -1.77136 -0.507161 -1.50042 -0.246927 -0.886747 0.443779 0.403664 -0.0794771 0.576868 -1.25723 0.643783 0.594685 -0.118211 0.424194 -3.45102 -1.21736 -0.622863 -0.219265 0.00558343 0.433809 1.25705 0.263417 0.568357 -1.74248 0.740367 -0.583025 0.352622 -0.305103 0.262624 -0.178496 -0.298974 0.0575156 0.231533 1.10592 -0.73359 0.317246 -0.897589 -0.973945 0.494638 0.643598 -2.05711 -0.0366701 -1.0615 -0.160956 -0.525382 0.359821 -1.54509 2.49662 -1.54785 -0.755846 -0.83514 -0.77244 -0.266884 1.50667 1.88736 2.04604 -1.19248 0.126276 1.09579 -0.789351 0.0301995 -0.878425 -0.439567 -0.486873 0.29533 -0.603165 0.817088 -1.32731 1.55283 0.219123 0.594835 0.268088 -1.51692 -0.236919 -0.990171 1.51414 -2.59838 1.11426 1.27968 -0.889761 -1.93741 0.173872 -2.16108 1.29886 0.441392 0.311922 2.34541 -1.248 -0.538492 0.592146 0.042345 0.290246 0.544928 -1.16383 0.642448 -0.511912 -0.539986 1.1389 -0.223306 -1.46078 0.250902 1.9974 -0.630635 -0.958025 0.282653 0.061663 0.230249 0.761137 0.512475 0.583108 -0.840476 -1.5605 -0.383426 1.16555 -1.37149 1.03419 1.78448 0.8596 0.535824 0.376521 -1.68934 1.26714 -0.104707 -0.78609 1.29683 -0.34752 -0.726585 0.387513 0.565065 0.217063 -0.908246 -0.237156 -0.140164 0.0459768 -0.333585 0.366328 -0.40835 -1.9319 1.23153 0.122274 -0.470229 0.748784 0.973473 0.0187009 0.658067 -0.639987 -0.828213 1.23821 -0.872973 0.388703 0.298925 -1.09754 1.24798 -0.607602 0.628044 -0.377055 -6.38546 -1.03209 -5.90699 0.478361 -1.14601 1.76986 -0.0187061 0.517279 0.786129 -0.735937 0.84829 -4.082 -0.24454 -4.19054 -0.59209 -0.969631 1.3707 -0.030874 -0.109574 0.181782 -0.200405 0.979819 -0.814501 -0.574278 -0.881648 0.240231 1.19754 0.246412 -0.0519865 -0.0928983 0.432925 -0.383856</Parameters>
    </MultilayerPerceptron>
    <UnscalingLayer>
        <UnscalingNeuronsNumber>0</UnscalingNeuronsNumber>
        <UnscalingMethod>MinimumMaximum</UnscalingMethod>
    </UnscalingLayer>
    <Outputs>
        <OutputsNumber>3</OutputsNumber>
        <Item Index="1">
            <Name>R</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="2">
            <Name>G</Name>
            <Units></Units>
            <Description></Description>
        </Item>
        <Item Index="3">
            <Name>B</Name>
            <Units></Units>
            <Description></Description>
        </Item>
    </Outputs>
</NeuralNetwork>
